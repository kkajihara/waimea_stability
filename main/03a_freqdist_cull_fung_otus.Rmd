---
title: "Cutoff Exploration (following Laura's Rmd)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, message=FALSE}

library(dplyr)
library(ggplot2)
library(ggpubr)
library(phyloseq)

```


```{r, echo=FALSE}

# read in abundance table
# sample IDS are cols, otus are rows
# ITS sample IDs has X appended to front

fung_otu_match <- readRDS("../intermediates/fungal_otu_table_matched_up.rds")
fung_meta_match <- readRDS("../intermediates/fungal_otu_metadata_matched_up.rds")

abun <- fung_otu_match # 34126 OTUs, 1372 samples

```

### By otus

Laura's guidance was for removing samples with low reads. To start, I will follow these steps for looking at otus.

```{r, echo=FALSE}
sampsums <- colSums(abun)
otusums <- rowSums(abun)
```


First, she looks at the summary of sequencing depths:

```{r, echo=FALSE}
# summary of sequencing depths
summary(otusums)

```


Then, checking how many otus have no reads and what the minimum non-zero sequencing depth is:

```{r, echo=FALSE}

# check how many samples have no reads and what the minimum non-zero sequencing depth is
paste("Number of otus with no reads:",sum(otusums==0))
paste("Minimum non-zero sequencing depth:",sort(otusums)[sum(otusums==0)+1])

```

As well as whether the maximum may be an outlier:
```{r, echo=FALSE}
# is the maximum an outlier?
tail(sort(otusums),5)
```


She said in her email that for rare otus, she usually only excludes those seen in only one sample (unless building a network). The other common option would be something "very low like 10 reads." Here we can look at how many otus we would exclude at either of those cutoffs.

Plot version:

```{r, echo=FALSE}

plot(sort(log1p(otusums)), pch=19, xlab="Number of otus", ylab="Log of Reads", main="otus by read abundance with cutoffs set at 1, 5, and 10 reads")
abline(h=log1p(1), lty=2, col="blue")
abline(h=log1p(5), lty=2, col="green")
abline(h=log1p(10), lty=2, col="magenta")
```


If we cull otus with fewer than 10 reads:
```{r, echo=FALSE}
# hypothetical cutoff at 10 reads
paste("We would remove this many otus:",sum(otusums<10))
paste0("Which is ", round((sum(otusums<10)/length(otusums))*100,2), "% of otus")
```

If we cull everything with 5 reads:
```{r, echo=FALSE}
# hypothetical cutoff at 1 reads
paste("We would remove this many otus:", sum(otusums<2))
paste0("Which is ", round((sum(otusums<2)/length(otusums))*100,2), "% of otus")
```
If we cull everything with 5 reads:
```{r, echo=FALSE}
# hypothetical cutoff at 1 reads
paste("We would remove this many otus:", sum(otusums<6))
paste0("Which is ", round((sum(otusums<6)/length(otusums))*100,2), "% of otus")
```

### By Samples

Summary of sequencing depths by sample:

```{r, echo=FALSE}
# summary of sequencing depths
summary(sampsums)

```


How many samples have no reads and what the minimum non-zero sequencing depth is:

```{r, echo=FALSE}

# check how many samples have no reads and what the minimum non-zero sequencing depth is
paste("Number of Samples with no reads:",sum(sampsums==0))
paste("Minimum non-zero sequencing depth:",sort(sampsums)[sum(sampsums==0)+1])

```

Checking the maximum:
```{r, echo=FALSE}
# is the maximum an outlier?
tail(sort(sampsums),5)
```


Looking at sequencing depth among controls:

```{r, echo=FALSE}

# subset metadata for controls only
neg_con_dat <- its_meta[grep("negative", its_meta$sample_type, ignore.case = T),]

# extract sequencing ids
control_ids <- unique(neg_con_dat$sequencing_id)

# subset abundance data by control sequencing ids
con_abun <- abun[,names(abun) %in% control_ids]

# remove empty otus
con_abun <- con_abun[rowSums(con_abun) > 0,]

controls <- colSums(con_abun)

```

```{r, echo=FALSE}

maxcntrl <- max(controls)

paste("Highest read depth among controls:",maxcntrl)

s1 <- sum(sampsums<=maxcntrl)
paste("Number of samples less than that:",s1)

```

```{r, echo=FALSE, fig.width=12, message=FALSE}

options(scipen=99999)

# read counts for negative controls
# negative_controls <- sort(controls[which(grepl("NegativeControl", con_dat$sample_type))])

n = as.data.frame(controls)

negs <- ggplot(n, aes(x=controls)) +
  geom_histogram(fill="lightgray", color="black") +
  stat_bin(bins = 100) +
  theme_classic() +
  labs(title="Reads of negative controls")
  

```

Back to Laura's steps:

If we only include samples that have more than 10% of the maximum, we would exclude this many:
```{r within10, echo=FALSE}
sum(sampsums<max(sampsums)/10)
```

Instead of using 10%, we can reduce that to the order of magnitude (number of zeros, so a 10,000 read cutoff). That would remove this many:
```{r withinOrder, echo=FALSE}
sum(sampsums<10000)
```

So let's assume the top 2 are outliers and work with the third highest sequencing depth. If we remove samples with less than 10% of this new maximum, we'd exclude this many:
```{r within10_2, echo=FALSE}
newmax <- tail(sort(sampsums))[3]
sum(sampsums<newmax/10)
```

And if we reduce this to the order of magnitude, we get a cut off of 1,000 reads, which is not uncommon. In this case we would remove this many samples:
```{r withinOrder_2, echo=FALSE}
sum(sampsums<1000)
```

Let's check a middle ground, just for fun. If we use a cut off of 5,000 reads, let's see what happens.
```{r under5000, echo=FALSE}
paste("We would remove this many samples:",sum(sampsums<5000))
paste0("Which is ", round((sum(sampsums<5000)/length(sampsums))*100,2), "% of samples")
```

Plot version:
Here we have cutoff lines at 1000 reads in pink, 100 reads in blue, and 10 reads in dark green.
```{r plots, echo=FALSE}
plot(sort(log1p(sampsums)), pch=19, xlab="Number of Samples", ylab="Log of Reads", main="Samples by read abundance with cutoffs set at 1000, 100, and 10 reads")
abline(h=log1p(190), lty=2, col="blue")
abline(h=log1p(1000), lty=2, col="magenta")
abline(h=log1p(10), lty=2, col="darkgreen")
```



If we cull samples with 1000 or fewer reads:
```{r, echo=FALSE}
# hypothetical cutoff at 1000 reads
paste("We would remove this many Samples:",sum(sampsums<1000))
paste0("Which is ", round((sum(sampsums<1000)/length(sampsums))*100,2), "% of Samples")
```

If we cull samples with 100 or fewer reads:
```{r, echo=FALSE}
# hypothetical cutoff at 100 reads
paste("We would remove this many Samples:",sum(sampsums<190))
paste0("Which is ", round((sum(sampsums<190)/length(sampsums))*100,2), "% of Samples")
```

If we cull samples with 10 or fewer reads:
```{r, echo=FALSE}
# hypothetical cutoff at 10 reads
paste("We would remove this many Samples:",sum(sampsums<10))
paste0("Which is ", round((sum(sampsums<10)/length(sampsums))*100,2), "% of Samples")
```


The 190 read line (blue) appears to be around a somewhat "break" in the data? 



```{r cull, echo=FALSE, message=FALSE}

# want to figure out where that natural break in the data is (somewhere around 100 reads per sample)
# adjusting the plot from earlier, it seems to be around 180 reads

# so, want to cull the abundance file (and then reduce the metadata and tax files by samples or otus that are no longer there)

culled_abun <- abun[,colSums(abun)>190] # 47 samples removed, 1325 remain
# remove otus with fewer than 5 reads
culled_abun <- culled_abun[rowSums(culled_abun)>5,] # 30596 OTUs removed, 30596 remain

saveRDS(culled_abun, "../intermediates/fung_culled_abun_otu_lauras_method_post_matchup.rds")


# cull tax table
# culled_tax <- tax[rownames(tax) %in% rownames(culled_abun),]
# 
# saveRDS(culled_tax, "../intermediates/culled_tax_table.rds")

# cull metadata
meta <- fung_meta_match
culled_meta <- meta[meta$x_seq_id %in% names(culled_abun),]

saveRDS(culled_meta, "../intermediates/fung_culled_metadata_otu_lauras_method_post_matchup.rds")


```








